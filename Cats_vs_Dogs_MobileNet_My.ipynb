{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats vs Dogs MobileNet My",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFKpTkr4cjMN"
      },
      "source": [
        "# **Classifying Cats or Dogs using Transfer Learning**\n",
        "  In this notebook,we will classify cats or dogs using a Pretrained MobileNet model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h0PGiiHSsO0"
      },
      "source": [
        "Install the required Packages!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpOwEDifOSs7",
        "outputId": "1107c23a-97fd-4e3d-d2be-3ecda3fbbf67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Collecting tensorflow_datasets\n",
            "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.5.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.6)\n",
            "Collecting etils[epath]\n",
            "  Downloading etils-0.6.0-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow_datasets) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.2)\n",
            "Installing collected packages: etils, toml, tensorflow-datasets\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed etils-0.6.0 tensorflow-datasets-4.6.0 toml-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IeyLyOgTbrE"
      },
      "source": [
        "Import the necessary Packages that are required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tSsVB9aOy2B"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94N2gFLQT5AK"
      },
      "source": [
        "**Loading the datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NXoEGkTpWs"
      },
      "source": [
        "\n",
        "Load the data and split it into train and validation sets!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq1SgeczPggQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d967a19-b5f6-47ca-9ed1-f200dc4c1355"
      },
      "source": [
        "(train_examples, validation_examples), info = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split = ('train[:80%]', 'train[80%:]'),\n",
        "    with_info = True,\n",
        "    as_supervised = True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to ~/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset cats_vs_dogs downloaded and prepared to ~/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phg0nnj2UG8c"
      },
      "source": [
        "# **Preprocessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lB489mySjfG"
      },
      "source": [
        "def format_image(image, label):\n",
        "  image = tf.image.resize(image, IMG_SIZE)/255.0\n",
        "  return image,label\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "train_batches = train_examples.cache().shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkEZFf2TUTN5"
      },
      "source": [
        "To use a pretrained model,we should install the tensorflow-hub package,in this we are going to use Google's mobilenet for getting better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwcSiANiV82A",
        "outputId": "537564f5-c2cf-446c-beff-9e3a95574503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow_hub"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bUnK4ACUjKC"
      },
      "source": [
        "# **Ge the MobileNet pretrained layer!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE + (3,)), include_top=False)"
      ],
      "metadata": {
        "id": "0IWK89k_yFnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c076455-2868-4472-d6b4-da6c8544b2fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10i5fGyDZkMJ"
      },
      "source": [
        "feature_extractor.trainable = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1vMNuO6VAZi"
      },
      "source": [
        "Train the pretrained model!!,just change the output layer,in this we are classifying cats vs dogs, as it is a binary classification,there are only two output layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW9zuc_TZ9od",
        "outputId": "d7ba767a-fdf4-4e49-95d3-0150870a6df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             feature_extractor,\n",
        "                             tf.keras.layers.GlobalMaxPooling2D(),\n",
        "                             layers.Dense(2, activation=\"softmax\")                           \n",
        "], name=\"cats_vs_dogs\")\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cats_vs_dogs\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " global_max_pooling2d (Globa  (None, 1280)             0         \n",
            " lMaxPooling2D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58FOBk5WCZG"
      },
      "source": [
        "# **Compile the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            monitor=\"val_loss\",\n",
        "                                            verbose=0,\n",
        "                                            save_best_only=True)"
      ],
      "metadata": {
        "id": "c4Xb9gnc7rZp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAm7oSReaXIX",
        "outputId": "b6e6279a-72ef-47d9-d633-d93f44aa8603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "EPOCHS = 5\n",
        "history = model.fit(train_batches,\n",
        "                    epochs = EPOCHS,\n",
        "                    validation_data = validation_batches,\n",
        "                    callbacks=[create_model_checkpoint(model_name=model.name)])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1164/1164 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9624"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_experiments/cats_vs_dogs/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_experiments/cats_vs_dogs/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1164/1164 [==============================] - 73s 49ms/step - loss: 0.1703 - accuracy: 0.9624 - val_loss: 0.2222 - val_accuracy: 0.9656\n",
            "Epoch 2/5\n",
            "1164/1164 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9789"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_experiments/cats_vs_dogs/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_experiments/cats_vs_dogs/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1164/1164 [==============================] - 44s 38ms/step - loss: 0.1197 - accuracy: 0.9789 - val_loss: 0.1014 - val_accuracy: 0.9819\n",
            "Epoch 3/5\n",
            "1164/1164 [==============================] - 28s 24ms/step - loss: 0.0964 - accuracy: 0.9811 - val_loss: 0.1101 - val_accuracy: 0.9807\n",
            "Epoch 4/5\n",
            "1164/1164 [==============================] - 27s 23ms/step - loss: 0.1075 - accuracy: 0.9818 - val_loss: 0.2772 - val_accuracy: 0.9639\n",
            "Epoch 5/5\n",
            "1164/1164 [==============================] - 28s 24ms/step - loss: 0.0888 - accuracy: 0.9838 - val_loss: 0.1147 - val_accuracy: 0.9811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvd7a3qLWP3x"
      },
      "source": [
        "We can see that it has a nearly 99% accuracy on validation examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viRzZiM-bE5V",
        "outputId": "b1b67c98-ff0f-48d6-e3af-775415fa155c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_names = np.array(info.features['label'].names)\n",
        "class_names"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['cat', 'dog'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tidoXl38VgH3"
      },
      "source": [
        "Predict for the next batch of images!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwwh8kebWfvy"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0rRtf4Ha-YO"
      },
      "source": [
        "Save the model in h5 format to use it later!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMSekXPCeTvG"
      },
      "source": [
        "# Load in the best saved model\n",
        "model_1 = tf.keras.models.load_model(\"/content/model_experiments/cats_vs_dogs\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n02TcCIfjZi",
        "outputId": "89f36cfb-8dbf-49ca-e6b1-c10c7bf493ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_1.evaluate(validation_batches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 [==============================] - 9s 29ms/step - loss: 0.1014 - accuracy: 0.9819\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10142698138952255, 0.9819432497024536]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxjwSm6cMCL"
      },
      "source": [
        "# Saving the model in .h5 format\n",
        "Save the trained reloaded model,to export it later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save(\"cats_vs_dogs.h5\")"
      ],
      "metadata": {
        "id": "ZDmi_x3g6rZ7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the testing data for benchmarking"
      ],
      "metadata": {
        "id": "OJdPtg0a4HTr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri2U9C9FiOus"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"/content/cats_vs_dogs.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = tf.keras.utils.image_dataset_from_directory(directory=\"/content/cats_vs_dogs\",\n",
        "                                              #color_mode=\"grayscale\",\n",
        "                                              image_size=IMG_SIZE,\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              label_mode=\"int\")"
      ],
      "metadata": {
        "id": "Jxc7GvLZtlnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2dd27e6-efcd-4020-eba3-5fdcc7cb0c5d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label"
      ],
      "metadata": {
        "id": "U2oHHbVbuXPO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = val_data.map(normalize_img)\n",
        "val_data"
      ],
      "metadata": {
        "id": "K6vRX9CnvCrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c00e512-27da-4697-897d-34d1e46204d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmark Accuracy"
      ],
      "metadata": {
        "id": "-9schCLd4R02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Accuracy of the validation data {model_1.evaluate(val_data)[1] * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "cpqvNqvtueVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdd7cd8-b99c-4769-8837-4fb3116f0cf7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 1s 34ms/step - loss: 0.1247 - accuracy: 0.9700\n",
            "The Accuracy of the validation data 97.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to read in benchmarking images"
      ],
      "metadata": {
        "id": "Op-vlwvu4VTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to infer the val images \n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "import PIL.Image\n",
        "\n",
        "def benchmark(val_dir, model, class_names=class_names, image_size=IMG_SIZE):\n",
        "  file_count = 0\n",
        "  infer_times = []\n",
        "  for (root, dirs, files) in os.walk(val_dir):\n",
        "    for name in files:\n",
        "      if name.endswith(\".jpg\"):\n",
        "        filename = os.path.join(root, name)\n",
        "        if file_count < 1 :\n",
        "          init_timer_start = time.time()\n",
        "          img = np.array(Image.open(filename).resize(IMG_SIZE))/255.\n",
        "          pred = model.predict(np.expand_dims(img, axis=0))\n",
        "          pred_class = class_names[int(np.argmax(pred[0]))]\n",
        "          init_timer_end = time.time()\n",
        "          init_timer = init_timer_end - init_timer_start\n",
        "          file_count+=1\n",
        "        else:\n",
        "          timer_start = time.time()\n",
        "          img = np.array(Image.open(filename).resize(IMG_SIZE))/255.\n",
        "          pred = model.predict(np.expand_dims(img, axis=0))\n",
        "          pred_class = class_names[int(np.argmax(pred[0]))]\n",
        "          timer_end = time.time()\n",
        "          infer_times.append((timer_end - timer_start))\n",
        "          file_count+=1\n",
        "\n",
        "  return init_timer, np.mean(infer_times), np.std(infer_times)"
      ],
      "metadata": {
        "id": "1r6k1fL7u-fE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_time, avg_time, std = benchmark(val_dir=\"/content/cats_vs_dogs\",\n",
        "          model=model_1)\n",
        "print(f\"The first image takes {init_time * 1000:.2f} ms\")\n",
        "print(f\"The average time taken per 99 images {avg_time * 1000:.2f} ms\")\n",
        "print(f\"The standard deviation of samples is {std * 1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "-GdBLDFtv-Au",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832ac931-aa77-42ba-db4a-0c8e363dbe84"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first image takes 972.27 ms\n",
            "The average time taken per 99 images 49.36 ms\n",
            "The standard deviation of samples is 3.84 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_time, avg_time, std = benchmark(val_dir=\"/content/cats_vs_dogs\",\n",
        "          model=model_1)\n",
        "print(f\"The first image takes {init_time * 1000:.2f} ms\")\n",
        "print(f\"The average time taken per 99 images {avg_time * 1000:.2f} ms\")\n",
        "print(f\"The standard deviation of samples is {std * 1000:.2f} ms\")"
      ],
      "metadata": {
        "id": "KCf7r0fxAo6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3dfcc81-6c5f-4357-ec40-a1fa6393f4c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first image takes 67.61 ms\n",
            "The average time taken per 99 images 50.54 ms\n",
            "The standard deviation of samples is 4.83 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lvunk5zyAr-6"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}